{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genie Enhancement v3 - Debug Notebook\n",
    "\n",
    "## 4-Stage Batch Apply Flow\n",
    "\n",
    "This notebook tests the v3 enhancement workflow:\n",
    "\n",
    "1. **Score** - Evaluate benchmarks on Genie Space\n",
    "2. **Plan** - Analyze failures, generate ALL fixes\n",
    "3. **Apply** - Apply ALL fixes in ONE batch update\n",
    "4. **Validate** - Re-score and check improvement\n",
    "\n",
    "## Key Difference from v2\n",
    "- v2: Apply fixes one-at-a-time with rollback\n",
    "- v3: Apply ALL fixes at once (batch)\n",
    "\n",
    "## Usage\n",
    "Run cells in order. Each section can be debugged independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Setup"
   ]
  },
  {
   "cell_type": "code",
   "source": "# IMPORTANT: Clear cached modules to ensure latest code is loaded\n# Run this cell first if you've updated the lib/ code\nimport sys\n\nmodules_to_remove = [m for m in sys.modules if m.startswith('lib')]\nfor m in modules_to_remove:\n    del sys.modules[m]\n\nprint(f\"Cleared {len(modules_to_remove)} cached lib modules\")\nprint(\"Now run the rest of the notebook to use fresh imports\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Project path setup\nimport sys\nimport os\nfrom pathlib import Path\n\n# Find project root\ncurrent_path = Path(os.getcwd())\nif current_path.name == 'genie_enhancer':\n    project_root = current_path\nelse:\n    project_root = current_path\n    while project_root.name != 'genie_enhancer' and project_root != project_root.parent:\n        project_root = project_root.parent\n\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\n# Configure logging for verbose output\nimport logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n    datefmt='%H:%M:%S'\n)\n\n# Set verbose logging for lib modules\nfor module in ['lib.genie_client', 'lib.scorer', 'lib.llm', 'lib.enhancer', 'lib.applier', 'lib.space_api']:\n    logging.getLogger(module).setLevel(logging.DEBUG)\n\nprint(f\"Project root: {project_root}\")\nprint(f\"Logging level: DEBUG (verbose mode enabled)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports from lib/\n",
    "try:\n",
    "    from lib.genie_client import GenieConversationalClient\n",
    "    print(\"✅ lib.genie_client\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ lib.genie_client: {e}\")\n",
    "\n",
    "try:\n",
    "    from lib.space_api import SpaceUpdater\n",
    "    print(\"✅ lib.space_api\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ lib.space_api: {e}\")\n",
    "\n",
    "try:\n",
    "    from lib.scorer import BenchmarkScorer\n",
    "    print(\"✅ lib.scorer\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ lib.scorer: {e}\")\n",
    "\n",
    "try:\n",
    "    from lib.benchmark_parser import BenchmarkLoader\n",
    "    print(\"✅ lib.benchmark_parser\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ lib.benchmark_parser: {e}\")\n",
    "\n",
    "try:\n",
    "    from lib.llm import DatabricksLLMClient\n",
    "    print(\"✅ lib.llm\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ lib.llm: {e}\")\n",
    "\n",
    "try:\n",
    "    from lib.sql import SQLExecutor\n",
    "    print(\"✅ lib.sql\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ lib.sql: {e}\")\n",
    "\n",
    "try:\n",
    "    from lib.enhancer import EnhancementPlanner\n",
    "    print(\"✅ lib.enhancer\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ lib.enhancer: {e}\")\n",
    "\n",
    "try:\n",
    "    from lib.applier import BatchApplier\n",
    "    print(\"✅ lib.applier\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ lib.applier: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full imports\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from lib.genie_client import GenieConversationalClient\n",
    "from lib.space_api import SpaceUpdater\n",
    "from lib.scorer import BenchmarkScorer\n",
    "from lib.benchmark_parser import BenchmarkLoader\n",
    "from lib.llm import DatabricksLLMClient\n",
    "from lib.sql import SQLExecutor\n",
    "from lib.enhancer import EnhancementPlanner\n",
    "from lib.applier import BatchApplier\n",
    "\n",
    "print(\"✅ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UPDATE THESE VALUES ===\n",
    "DATABRICKS_HOST = \"your-workspace.cloud.databricks.com\"\n",
    "DATABRICKS_TOKEN = \"YOUR_TOKEN_HERE\"\n",
    "GENIE_SPACE_ID = \"your-space-id\"\n",
    "WAREHOUSE_ID = \"your-warehouse-id\"  # For metric views\n",
    "LLM_ENDPOINT = \"databricks-claude-sonnet-4\"\n",
    "\n",
    "# Target score\n",
    "TARGET_SCORE = 0.90\n",
    "\n",
    "print(f\"Host: {DATABRICKS_HOST}\")\n",
    "print(f\"Space ID: {GENIE_SPACE_ID}\")\n",
    "print(f\"Warehouse: {WAREHOUSE_ID}\")\n",
    "print(f\"LLM: {LLM_ENDPOINT}\")\n",
    "print(f\"Target: {TARGET_SCORE:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LLM Client (with rate limit protection)\nprint(\"Initializing LLM Client...\")\nllm_client = DatabricksLLMClient(\n    host=DATABRICKS_HOST,\n    token=DATABRICKS_TOKEN,\n    endpoint_name=LLM_ENDPOINT,\n    request_delay=10.0,          # 10s delay between requests\n    rate_limit_base_delay=90.0   # 90s base delay on rate limit (90, 180, 360s...)\n)\n\nif llm_client.test_connection():\n    print(\"✅ LLM Client connected\")\n    print(\"   - Request delay: 10s between calls\")\n    print(\"   - Rate limit retry: 90s base (exponential backoff)\")\nelse:\n    print(\"❌ LLM connection failed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Client\n",
    "print(\"Initializing LLM Client...\")\n",
    "llm_client = DatabricksLLMClient(\n",
    "    host=DATABRICKS_HOST,\n",
    "    token=DATABRICKS_TOKEN,\n",
    "    endpoint_name=LLM_ENDPOINT\n",
    ")\n",
    "\n",
    "if llm_client.test_connection():\n",
    "    print(\"✅ LLM Client connected\")\n",
    "else:\n",
    "    print(\"❌ LLM connection failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space API (for export/import)\n",
    "print(\"Initializing Space API...\")\n",
    "space_api = SpaceUpdater(\n",
    "    host=DATABRICKS_HOST,\n",
    "    token=DATABRICKS_TOKEN\n",
    ")\n",
    "print(\"✅ Space API initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Executor (for metric views)\n",
    "print(\"Initializing SQL Executor...\")\n",
    "sql_executor = SQLExecutor(\n",
    "    host=DATABRICKS_HOST,\n",
    "    token=DATABRICKS_TOKEN,\n",
    "    warehouse_id=WAREHOUSE_ID\n",
    ")\n",
    "print(\"✅ SQL Executor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark Scorer (verbose config)\nprint(\"Initializing Scorer...\")\nscorer = BenchmarkScorer(\n    genie_client=genie_client,\n    llm_client=llm_client,\n    sql_executor=sql_executor,\n    config={\n        \"question_timeout\": 120,\n        \"question_delay\": 3.0,      # Delay between questions\n        \"error_delay\": 5.0,         # Extra delay after errors\n        \"parallel_workers\": 0,      # 0 = sequential (easier to debug)\n    }\n)\nprint(\"✅ Scorer initialized\")\nprint(\"   - Sequential mode (parallel_workers=0)\")\nprint(\"   - Question delay: 3s\")\nprint(\"   - Timeout: 120s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Load Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmarks\n",
    "benchmark_file = project_root / \"benchmarks\" / \"benchmarks.json\"\n",
    "print(f\"Loading from: {benchmark_file}\")\n",
    "\n",
    "loader = BenchmarkLoader(str(benchmark_file))\n",
    "all_benchmarks = loader.load()\n",
    "print(f\"✅ Loaded {len(all_benchmarks)} benchmarks\")\n",
    "\n",
    "# Show first few\n",
    "for i, b in enumerate(all_benchmarks[:3]):\n",
    "    print(f\"  {i+1}. {b['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Filter for faster testing\n",
    "USE_SUBSET = True  # Set to False for full run\n",
    "\n",
    "if USE_SUBSET:\n",
    "    benchmarks = all_benchmarks[:5]  # First 5 only\n",
    "    print(f\"⚠️ TEST MODE: Using {len(benchmarks)} benchmarks\")\n",
    "else:\n",
    "    benchmarks = all_benchmarks\n",
    "    print(f\"FULL MODE: Using {len(benchmarks)} benchmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STAGE 1: SCORE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run scoring (verbose output)\nprint(\"=\"*60)\nprint(\"STAGE 1: SCORING BENCHMARKS\")\nprint(\"=\"*60)\nprint()\n\nstart_time = datetime.now()\nscore_results = scorer.score(benchmarks)\nduration = (datetime.now() - start_time).total_seconds()\n\nprint()\nprint(\"=\"*60)\nprint(\"SCORING COMPLETE\")\nprint(\"=\"*60)\nprint(f\"Score: {score_results['score']:.1%}\")\nprint(f\"Passed: {score_results['passed']}/{score_results['total']}\")\nprint(f\"Failed: {score_results['failed']}\")\nprint(f\"Duration: {duration:.1f}s\")\nprint(f\"Avg per question: {duration/len(benchmarks):.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show detailed results for each benchmark\nprint(\"=\"*60)\nprint(\"DETAILED RESULTS\")\nprint(\"=\"*60)\nprint()\n\nfor i, r in enumerate(score_results['results'], 1):\n    status = \"✅ PASS\" if r['passed'] else \"❌ FAIL\"\n    print(f\"[{i}/{len(score_results['results'])}] {status}\")\n    print(f\"   Question: {r['question'][:70]}...\")\n    \n    if r['passed']:\n        print(f\"   Genie SQL: {(r.get('genie_sql') or 'N/A')[:60]}...\")\n    else:\n        print(f\"   Category: {r.get('failure_category', 'unknown')}\")\n        print(f\"   Reason: {(r.get('failure_reason') or 'N/A')[:80]}\")\n        if r.get('genie_sql'):\n            print(f\"   Genie SQL: {r['genie_sql'][:60]}...\")\n        if r.get('expected_sql'):\n            print(f\"   Expected:  {r['expected_sql'][:60]}...\")\n    \n    print(f\"   Response time: {r.get('response_time', 0):.1f}s\")\n    print()\n\n# Summary\nfailed_results = [r for r in score_results['results'] if not r['passed']]\nprint(\"=\"*60)\nprint(f\"SUMMARY: {len(failed_results)} failures to analyze\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STAGE 2: PLAN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current space config\n",
    "print(\"Exporting current space config...\")\n",
    "space_config = space_api.export_space(GENIE_SPACE_ID)\n",
    "print(f\"✅ Config loaded\")\n",
    "print(f\"   Tables: {len(space_config.get('data_sources', {}).get('tables', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate enhancement plan (verbose)\nprint(\"=\"*60)\nprint(\"STAGE 2: GENERATING ENHANCEMENT PLAN\")\nprint(\"=\"*60)\nprint()\nprint(f\"Analyzing {len(failed_results)} failures...\")\nprint(f\"Categories: metric_view, metadata, sample_query, instruction\")\nprint(f\"Parallel workers: 1 (sequential to avoid rate limits)\")\nprint()\n\nplan_start = datetime.now()\ngrouped_fixes = planner.generate_plan(\n    failed_benchmarks=failed_results,\n    space_config=space_config,\n    parallel_workers=1  # Sequential to avoid rate limits\n)\nplan_duration = (datetime.now() - plan_start).total_seconds()\n\ntotal_fixes = sum(len(f) for f in grouped_fixes.values())\nprint()\nprint(\"=\"*60)\nprint(\"PLAN GENERATION COMPLETE\")\nprint(\"=\"*60)\nprint(f\"Total fixes generated: {total_fixes}\")\nprint(f\"Duration: {plan_duration:.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate enhancement plan (verbose)\nprint(\"=\"*60)\nprint(\"STAGE 2: GENERATING ENHANCEMENT PLAN\")\nprint(\"=\"*60)\nprint()\nprint(f\"Analyzing {len(failed_results)} failures...\")\nprint(f\"Categories: metric_view, metadata, sample_query, instruction\")\nprint(f\"Parallel workers: 2\")\nprint()\n\nplan_start = datetime.now()\ngrouped_fixes = planner.generate_plan(\n    failed_benchmarks=failed_results,\n    space_config=space_config,\n    parallel_workers=2  # Reduce for debugging\n)\nplan_duration = (datetime.now() - plan_start).total_seconds()\n\ntotal_fixes = sum(len(f) for f in grouped_fixes.values())\nprint()\nprint(\"=\"*60)\nprint(\"PLAN GENERATION COMPLETE\")\nprint(\"=\"*60)\nprint(f\"Total fixes generated: {total_fixes}\")\nprint(f\"Duration: {plan_duration:.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show all fixes by category (verbose)\nprint(\"=\"*60)\nprint(\"GENERATED FIXES BY CATEGORY\")\nprint(\"=\"*60)\n\nfor category in [\"metric_view\", \"metadata\", \"sample_query\", \"instruction\"]:\n    fixes = grouped_fixes.get(category, [])\n    print(f\"\\n{'='*40}\")\n    print(f\"{category.upper()} ({len(fixes)} fixes)\")\n    print(\"=\"*40)\n    \n    if not fixes:\n        print(\"  (none)\")\n        continue\n    \n    for i, fix in enumerate(fixes, 1):\n        fix_type = fix.get('type', 'unknown')\n        print(f\"\\n  [{i}] {fix_type}\")\n        \n        # Show fix-specific details\n        if fix_type == 'add_synonym':\n            print(f\"      Table: {fix.get('table')}\")\n            print(f\"      Column: {fix.get('column')}\")\n            print(f\"      Synonym: '{fix.get('synonym')}'\")\n        elif fix_type == 'delete_synonym':\n            print(f\"      Table: {fix.get('table')}\")\n            print(f\"      Column: {fix.get('column')}\")\n            print(f\"      Remove: '{fix.get('synonym')}'\")\n        elif fix_type == 'add_column_description':\n            print(f\"      Table: {fix.get('table')}\")\n            print(f\"      Column: {fix.get('column')}\")\n            print(f\"      Description: {(fix.get('description') or '')[:60]}...\")\n        elif fix_type == 'add_table_description':\n            print(f\"      Table: {fix.get('table')}\")\n            print(f\"      Description: {(fix.get('description') or '')[:60]}...\")\n        elif fix_type == 'add_example_query':\n            print(f\"      Pattern: {fix.get('pattern_name')}\")\n            print(f\"      Question: {(fix.get('question') or '')[:50]}...\")\n            print(f\"      SQL: {(fix.get('sql') or '')[:50]}...\")\n        elif fix_type == 'create_metric_view':\n            print(f\"      View: {fix.get('catalog')}.{fix.get('schema')}.{fix.get('metric_view_name')}\")\n            print(f\"      SQL: {(fix.get('sql') or '')[:60]}...\")\n        elif fix_type == 'update_text_instruction':\n            print(f\"      Text: {(fix.get('instruction_text') or '')[:80]}...\")\n        \n        # Show source failure\n        source = fix.get('source_failure', {})\n        if source:\n            print(f\"      Source: {source.get('question', '')[:40]}...\")\n\nprint()\nprint(\"=\"*60)\nprint(f\"TOTAL: {total_fixes} fixes ready to apply\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STAGE 3: APPLY (Batch)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Batch Applier\n",
    "print(\"Initializing Batch Applier...\")\n",
    "applier = BatchApplier(\n",
    "    space_api=space_api,\n",
    "    sql_executor=sql_executor,\n",
    "    config={\n",
    "        \"catalog\": \"sandbox\",\n",
    "        \"schema\": \"genie_enhancement\"\n",
    "    }\n",
    ")\n",
    "print(\"✅ Applier initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DRY RUN first (verbose)\nDRY_RUN = True  # Set to False to actually apply\n\nprint(\"=\"*60)\nprint(f\"STAGE 3: APPLY ALL FIXES {'(DRY RUN)' if DRY_RUN else '(LIVE)'}\")\nprint(\"=\"*60)\nprint()\n\nif DRY_RUN:\n    print(\"DRY RUN MODE: Changes will be simulated, not applied\")\nelse:\n    print(\"LIVE MODE: Changes WILL be applied to the Genie Space!\")\nprint()\n\napply_start = datetime.now()\napply_result = applier.apply_all(\n    space_id=GENIE_SPACE_ID,\n    grouped_fixes=grouped_fixes,\n    dry_run=DRY_RUN\n)\napply_duration = (datetime.now() - apply_start).total_seconds()\n\nprint()\nprint(\"=\"*60)\nprint(\"APPLY COMPLETE\")\nprint(\"=\"*60)\nprint(f\"Applied: {len(apply_result['applied'])}\")\nprint(f\"Failed: {len(apply_result['failed'])}\")\nprint(f\"Duration: {apply_duration:.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show applied fixes\n",
    "print(\"\\n✅ Applied Fixes:\")\n",
    "for i, fix in enumerate(apply_result['applied'][:10], 1):\n",
    "    print(f\"  {i}. {fix.get('type')}\")\n",
    "\n",
    "if apply_result['failed']:\n",
    "    print(\"\\n❌ Failed Fixes:\")\n",
    "    for i, fix in enumerate(apply_result['failed'], 1):\n",
    "        print(f\"  {i}. {fix.get('type')}: {fix.get('error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIVE RUN (uncomment to execute)\n",
    "# WARNING: This will modify your Genie Space!\n",
    "\n",
    "# print(\"Applying fixes for real...\")\n",
    "# apply_result = applier.apply_all(\n",
    "#     space_id=GENIE_SPACE_ID,\n",
    "#     grouped_fixes=grouped_fixes,\n",
    "#     dry_run=False\n",
    "# )\n",
    "# print(f\"Applied: {len(apply_result['applied'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STAGE 4: VALIDATE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Genie indexing (only if not dry run)\n",
    "INDEXING_WAIT = 60  # seconds\n",
    "\n",
    "if not DRY_RUN and len(apply_result['applied']) > 0:\n",
    "    print(f\"Waiting {INDEXING_WAIT}s for Genie indexing...\")\n",
    "    time.sleep(INDEXING_WAIT)\n",
    "    print(\"✅ Wait complete\")\n",
    "else:\n",
    "    print(\"Skipping wait (dry run or no changes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Re-score benchmarks (verbose)\nprint(\"=\"*60)\nprint(\"STAGE 4: VALIDATING RESULTS\")\nprint(\"=\"*60)\nprint()\nprint(\"Re-scoring all benchmarks...\")\nprint()\n\nvalidate_start = datetime.now()\nfinal_results = scorer.score(benchmarks)\nvalidate_duration = (datetime.now() - validate_start).total_seconds()\n\ninitial_score = score_results['score']\nfinal_score = final_results['score']\nimprovement = final_score - initial_score\n\nprint()\nprint(\"=\"*60)\nprint(\"VALIDATION COMPLETE\")\nprint(\"=\"*60)\nprint()\nprint(f\"Initial Score:  {initial_score:.1%} ({score_results['passed']}/{score_results['total']} passed)\")\nprint(f\"Final Score:    {final_score:.1%} ({final_results['passed']}/{final_results['total']} passed)\")\nprint(f\"Improvement:    {improvement:+.1%}\")\nprint(f\"Target:         {TARGET_SCORE:.1%}\")\nprint()\nprint(f\"Validation duration: {validate_duration:.1f}s\")\nprint()\n\nif final_score >= TARGET_SCORE:\n    print(\"=\"*60)\n    print(\"TARGET REACHED!\")\n    print(\"=\"*60)\nelif improvement > 0:\n    print(\"=\"*60)\n    print(f\"IMPROVED but need another loop (gap: {TARGET_SCORE - final_score:.1%})\")\n    print(\"=\"*60)\nelse:\n    print(\"=\"*60)\n    print(\"NO IMPROVEMENT - check fix quality\")\n    print(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Debug Utilities\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Genie API directly\n",
    "test_question = \"What tables are available?\"\n",
    "print(f\"Testing Genie: {test_question}\")\n",
    "\n",
    "response = genie_client.ask(test_question, timeout=60)\n",
    "print(f\"Status: {response['status']}\")\n",
    "if response.get('sql'):\n",
    "    print(f\"SQL: {response['sql'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LLM directly\n",
    "test_prompt = \"Say 'Hello, Genie Enhancement is working!'\"\n",
    "print(f\"Testing LLM...\")\n",
    "\n",
    "response = llm_client.generate(test_prompt, max_tokens=50)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export current config to JSON\n",
    "output_file = \"debug_space_config.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(space_config, f, indent=2)\n",
    "print(f\"✅ Config saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export fixes to JSON\n",
    "output_file = \"debug_fixes.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(grouped_fixes, f, indent=2, default=str)\n",
    "print(f\"✅ Fixes saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}