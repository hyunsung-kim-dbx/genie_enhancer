{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genie Enhancement v3 - Debug Notebook\n",
    "\n",
    "## Three-Space Architecture with Batch Apply\n",
    "\n",
    "**Safe enhancement workflow:**\n",
    "1. Clone production ‚Üí dev-working + dev-best\n",
    "2. Score benchmarks on dev-working (baseline)\n",
    "3. Apply ALL fixes at once to dev-working\n",
    "4. Wait for indexing\n",
    "5. Score again and compare\n",
    "6. Promote dev-working ‚Üí production (user decision)\n",
    "7. Cleanup dev spaces\n",
    "\n",
    "**Key Safety Features:**\n",
    "- Production is NEVER modified directly\n",
    "- All changes tested on dev-working first\n",
    "- User controls final promotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Clear cached modules to ensure latest code is loaded\n",
    "import sys\n",
    "\n",
    "modules_to_remove = [m for m in sys.modules if m.startswith('lib')]\n",
    "for m in modules_to_remove:\n",
    "    del sys.modules[m]\n",
    "\n",
    "print(f\"Cleared {len(modules_to_remove)} cached lib modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project path setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root\n",
    "current_path = Path(os.getcwd())\n",
    "if current_path.name == 'genie_enhancer':\n",
    "    project_root = current_path\n",
    "else:\n",
    "    project_root = current_path\n",
    "    while project_root.name != 'genie_enhancer' and project_root != project_root.parent:\n",
    "        project_root = project_root.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "\n",
    "# Verbose logging for lib modules\n",
    "for module in ['lib.genie_client', 'lib.scorer', 'lib.llm', 'lib.enhancer', \n",
    "               'lib.applier', 'lib.space_api', 'lib.space_cloner']:\n",
    "    logging.getLogger(module).setLevel(logging.DEBUG)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Logging: DEBUG mode enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport json\nimport time\nfrom datetime import datetime\n\nfrom lib.genie_client import GenieConversationalClient\nfrom lib.space_cloner import SpaceCloner\nfrom lib.scorer import BenchmarkScorer\nfrom lib.benchmark_parser import BenchmarkLoader\nfrom lib.llm import DatabricksLLMClient\nfrom lib.sql import SQLExecutor\nfrom lib.category_enhancer import CategoryEnhancer  # NEW: Category-based analysis\nfrom lib.applier import BatchApplier\n\nprint(\"‚úÖ All imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UPDATE THESE VALUES ===\n",
    "DATABRICKS_HOST = \"your-workspace.cloud.databricks.com\"\n",
    "DATABRICKS_TOKEN = \"YOUR_TOKEN_HERE\"\n",
    "GENIE_SPACE_ID = \"your-space-id\"  # Production space to enhance\n",
    "WAREHOUSE_ID = \"your-warehouse-id\"  # For metric views\n",
    "LLM_ENDPOINT = \"databricks-claude-sonnet-4\"\n",
    "\n",
    "# Enhancement settings\n",
    "TARGET_SCORE = 0.90\n",
    "INDEXING_WAIT = 60  # seconds to wait after applying all fixes\n",
    "\n",
    "print(f\"Host: {DATABRICKS_HOST}\")\n",
    "print(f\"Production Space: {GENIE_SPACE_ID}\")\n",
    "print(f\"Warehouse: {WAREHOUSE_ID}\")\n",
    "print(f\"LLM: {LLM_ENDPOINT}\")\n",
    "print(f\"Target: {TARGET_SCORE:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space Cloner (for three-space architecture)\n",
    "print(\"Initializing Space Cloner...\")\n",
    "space_cloner = SpaceCloner(\n",
    "    host=DATABRICKS_HOST,\n",
    "    token=DATABRICKS_TOKEN\n",
    ")\n",
    "print(\"‚úÖ Space Cloner initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Client (with rate limit protection)\n",
    "print(\"Initializing LLM Client...\")\n",
    "llm_client = DatabricksLLMClient(\n",
    "    host=DATABRICKS_HOST,\n",
    "    token=DATABRICKS_TOKEN,\n",
    "    endpoint_name=LLM_ENDPOINT,\n",
    "    request_delay=10.0,          # 10s delay between requests\n",
    "    rate_limit_base_delay=90.0   # 90s base on rate limit\n",
    ")\n",
    "\n",
    "if llm_client.test_connection():\n",
    "    print(\"‚úÖ LLM Client connected\")\n",
    "    print(\"   - Request delay: 10s\")\n",
    "    print(\"   - Rate limit backoff: 90s base\")\n",
    "else:\n",
    "    print(\"‚ùå LLM connection failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Executor (for metric views)\n",
    "print(\"Initializing SQL Executor...\")\n",
    "sql_executor = SQLExecutor(\n",
    "    host=DATABRICKS_HOST,\n",
    "    token=DATABRICKS_TOKEN,\n",
    "    warehouse_id=WAREHOUSE_ID\n",
    ")\n",
    "print(\"‚úÖ SQL Executor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmarks\n",
    "benchmark_file = project_root / \"benchmarks\" / \"fixed_benchmark_final.json\"\n",
    "print(f\"Loading from: {benchmark_file}\")\n",
    "\n",
    "loader = BenchmarkLoader(str(benchmark_file))\n",
    "all_benchmarks = loader.load()\n",
    "print(f\"‚úÖ Loaded {len(all_benchmarks)} benchmarks\")\n",
    "\n",
    "# Preview\n",
    "for i, b in enumerate(all_benchmarks[:3]):\n",
    "    print(f\"  {i+1}. {b['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use subset for faster testing\n",
    "USE_SUBSET = True  # Set to False for full run\n",
    "\n",
    "if USE_SUBSET:\n",
    "    benchmarks = all_benchmarks[:5]  # First 5 only\n",
    "    print(f\"‚ö†Ô∏è TEST MODE: Using {len(benchmarks)} benchmarks\")\n",
    "else:\n",
    "    benchmarks = all_benchmarks\n",
    "    print(f\"FULL MODE: Using {len(benchmarks)} benchmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Setup Three-Space Architecture\n",
    "\n",
    "This creates:\n",
    "- **Production** - Original space (never modified)\n",
    "- **Dev-Working** - Where changes are tested\n",
    "- **Dev-Best** - Backup of production (for rollback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SETTING UP THREE-SPACE ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Production Space: {GENIE_SPACE_ID}\")\n",
    "print(\"Creating dev-working and dev-best clones...\")\n",
    "print()\n",
    "\n",
    "setup_result = space_cloner.setup_three_spaces(\n",
    "    production_space_id=GENIE_SPACE_ID\n",
    ")\n",
    "\n",
    "if setup_result['success']:\n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(\"THREE-SPACE ARCHITECTURE READY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Production:   {setup_result['production_id']}\")\n",
    "    print(f\"Dev-Working:  {setup_result['dev_working_id']}\")\n",
    "    print(f\"Dev-Best:     {setup_result['dev_best_id']}\")\n",
    "    \n",
    "    # Store for later\n",
    "    PRODUCTION_ID = setup_result['production_id']\n",
    "    DEV_WORKING_ID = setup_result['dev_working_id']\n",
    "    DEV_BEST_ID = setup_result['dev_best_id']\n",
    "    INITIAL_CONFIG = setup_result['initial_config']\n",
    "else:\n",
    "    print(f\"‚ùå Setup failed: {setup_result['error']}\")\n",
    "    raise RuntimeError(\"Three-space setup failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Initialize Scorer and Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genie Client pointing to DEV-WORKING space\n",
    "print(\"Initializing Genie Client for dev-working space...\")\n",
    "genie_client = GenieConversationalClient(\n",
    "    host=DATABRICKS_HOST,\n",
    "    token=DATABRICKS_TOKEN,\n",
    "    space_id=DEV_WORKING_ID,  # Point to dev-working, NOT production\n",
    "    verbose=True\n",
    ")\n",
    "print(f\"‚úÖ Genie Client initialized (space: {DEV_WORKING_ID[:16]}...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Scorer\n",
    "print(\"Initializing Scorer...\")\n",
    "scorer = BenchmarkScorer(\n",
    "    genie_client=genie_client,\n",
    "    llm_client=llm_client,\n",
    "    sql_executor=sql_executor,\n",
    "    config={\n",
    "        \"question_timeout\": 120,\n",
    "        \"question_delay\": 3.0,\n",
    "        \"error_delay\": 5.0,\n",
    "        \"parallel_workers\": 0,  # Sequential for debugging\n",
    "    }\n",
    ")\n",
    "print(\"‚úÖ Scorer initialized (sequential mode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Category-Based Enhancement Planner (NEW!)\n# Uses 9 LLM calls total instead of N failures √ó 3 categories\nprint(\"Initializing Category Enhancer...\")\nprompts_dir = project_root / \"prompts\"\nplanner = CategoryEnhancer(llm_client, prompts_dir)\nprint(\"‚úÖ Category Enhancer initialized\")\nprint(\"   - 9 fix categories (constant LLM calls)\")\nprint(\"   - Categories: instruction, metadata, sample_queries, sql_snippets, join_specs\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Applier (applies to dev-working via space_cloner)\n",
    "print(\"Initializing Batch Applier...\")\n",
    "applier = BatchApplier(\n",
    "    space_api=space_cloner,  # Use space_cloner as the API\n",
    "    sql_executor=sql_executor,\n",
    "    config={\n",
    "        \"catalog\": \"sandbox\",\n",
    "        \"schema\": \"genie_enhancement\"\n",
    "    }\n",
    ")\n",
    "print(\"‚úÖ Batch Applier initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ Initial Scoring (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"INITIAL SCORING (baseline on dev-working)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "start_time = datetime.now()\n",
    "initial_results = scorer.score(benchmarks)\n",
    "duration = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "initial_score = initial_results['score']\n",
    "\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE SCORING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline Score: {initial_score:.1%}\")\n",
    "print(f\"Passed: {initial_results['passed']}/{initial_results['total']}\")\n",
    "print(f\"Failed: {initial_results['failed']}\")\n",
    "print(f\"Duration: {duration:.1f}s\")\n",
    "\n",
    "# Check if already at target\n",
    "if initial_score >= TARGET_SCORE:\n",
    "    print()\n",
    "    print(\"üéâ Already at target score! No enhancement needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show failed benchmarks\n",
    "failed_results = [r for r in initial_results['results'] if not r['passed']]\n",
    "\n",
    "print(f\"\\n‚ùå Failed Benchmarks ({len(failed_results)}):\\n\")\n",
    "for i, r in enumerate(failed_results, 1):\n",
    "    print(f\"{i}. {r['question'][:60]}...\")\n",
    "    print(f\"   Category: {r.get('failure_category', 'unknown')}\")\n",
    "    if r.get('failure_reason'):\n",
    "        print(f\"   Reason: {r['failure_reason'][:80]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ Generate Enhancement Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"GENERATING ENHANCEMENT PLAN (Category-Based)\")\nprint(\"=\"*60)\nprint()\nprint(f\"Analyzing {len(failed_results)} failures...\")\nprint(\"9 Categories: instruction_fix, metadata_add/delete, sample_queries_add/delete,\")\nprint(\"              sql_snippets_add/delete, join_specs_add/delete\")\nprint()\n\nplan_start = datetime.now()\ngrouped_fixes = planner.generate_plan(\n    failed_benchmarks=failed_results,\n    space_config=INITIAL_CONFIG,\n    parallel_workers=3  # Can run categories in parallel\n)\nplan_duration = (datetime.now() - plan_start).total_seconds()\n\n# New 9 categories\nFIX_CATEGORIES = [\n    'instruction_fix',\n    'join_specs_delete', 'join_specs_add',\n    'sql_snippets_delete', 'sql_snippets_add',\n    'metadata_delete', 'metadata_add',\n    'sample_queries_delete', 'sample_queries_add',\n]\n\ntotal_fixes = sum(len(grouped_fixes.get(cat, [])) for cat in FIX_CATEGORIES)\n\nprint()\nprint(\"=\"*60)\nprint(\"PLAN GENERATION COMPLETE\")\nprint(\"=\"*60)\nprint(f\"Total fixes: {total_fixes}\")\nprint(f\"Duration: {plan_duration:.1f}s\")\nprint(f\"LLM calls: 9 (constant, regardless of failure count)\")\n\nfor category in FIX_CATEGORIES:\n    count = len(grouped_fixes.get(category, []))\n    if count > 0:\n        print(f\"  - {category}: {count}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preview fixes by category\nprint(\"\\nFix Preview:\")\nprint(\"-\"*40)\n\nfor category in FIX_CATEGORIES:\n    fixes = grouped_fixes.get(category, [])\n    if fixes:\n        print(f\"\\n{category.upper()} ({len(fixes)} fixes):\")\n        for i, fix in enumerate(fixes[:5], 1):  # Show first 5\n            fix_type = fix.get('type', 'unknown')\n            # Metadata fixes\n            if fix_type == 'add_synonym':\n                print(f\"  {i}. {fix_type}: {fix.get('table')}.{fix.get('column')} ‚Üí '{fix.get('synonym')}'\")\n            elif fix_type == 'delete_synonym':\n                print(f\"  {i}. {fix_type}: {fix.get('table')}.{fix.get('column')} ‚úó '{fix.get('synonym')}'\")\n            elif fix_type in ('add_column_description', 'add_table_description'):\n                print(f\"  {i}. {fix_type}: {fix.get('table')}\")\n            # Sample query fixes\n            elif fix_type == 'add_example_query':\n                print(f\"  {i}. {fix_type}: {fix.get('pattern_name', 'N/A')}\")\n            elif fix_type == 'delete_example_query':\n                print(f\"  {i}. {fix_type}: {fix.get('pattern_name', fix.get('id', 'N/A'))}\")\n            # Instruction fixes\n            elif fix_type == 'update_text_instruction':\n                print(f\"  {i}. {fix_type}\")\n            # SQL snippet fixes\n            elif fix_type in ('add_filter', 'add_expression', 'add_measure'):\n                print(f\"  {i}. {fix_type}: {fix.get('display_name', fix.get('alias', 'N/A'))}\")\n            elif fix_type in ('delete_filter', 'delete_expression', 'delete_measure'):\n                print(f\"  {i}. {fix_type}: {fix.get('display_name', fix.get('id', 'N/A'))}\")\n            # Join spec fixes\n            elif fix_type == 'add_join_spec':\n                print(f\"  {i}. {fix_type}: {fix.get('left_table')} ‚Üî {fix.get('right_table')}\")\n            elif fix_type == 'delete_join_spec':\n                print(f\"  {i}. {fix_type}: {fix.get('left_table')} ‚Üî {fix.get('right_table')}\")\n            else:\n                print(f\"  {i}. {fix_type}\")\n        if len(fixes) > 5:\n            print(f\"  ... and {len(fixes) - 5} more\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9Ô∏è‚É£ Apply ALL Fixes at Once (Batch)\n",
    "\n",
    "This applies ALL fixes to dev-working in one batch:\n",
    "1. Apply all fixes at once\n",
    "2. Wait for Genie indexing\n",
    "3. Score and compare with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"APPLYING ALL FIXES (BATCH MODE)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Total fixes to apply: {total_fixes}\")\n",
    "print(f\"Target space: dev-working ({DEV_WORKING_ID[:16]}...)\")\n",
    "print()\n",
    "\n",
    "apply_start = datetime.now()\n",
    "apply_result = applier.apply_all(\n",
    "    space_id=DEV_WORKING_ID,  # Apply to dev-working, NOT production\n",
    "    grouped_fixes=grouped_fixes,\n",
    "    dry_run=False  # Actually apply\n",
    ")\n",
    "apply_duration = (datetime.now() - apply_start).total_seconds()\n",
    "\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"BATCH APPLY COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Applied: {len(apply_result['applied'])}\")\n",
    "print(f\"Failed: {len(apply_result['failed'])}\")\n",
    "print(f\"Duration: {apply_duration:.1f}s\")\n",
    "\n",
    "if apply_result['failed']:\n",
    "    print(\"\\n‚ùå Failed fixes:\")\n",
    "    for fix in apply_result['failed'][:5]:\n",
    "        print(f\"  - {fix.get('type')}: {fix.get('error', 'N/A')[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Genie indexing\n",
    "print(f\"\\nWaiting {INDEXING_WAIT}s for Genie to index changes...\")\n",
    "for i in range(INDEXING_WAIT, 0, -10):\n",
    "    print(f\"  {i}s remaining...\")\n",
    "    time.sleep(10)\n",
    "print(\"‚úÖ Indexing wait complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîü Validation Scoring (After Fixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"VALIDATION SCORING (after fixes)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "val_start = datetime.now()\n",
    "final_results = scorer.score(benchmarks)\n",
    "val_duration = (datetime.now() - val_start).total_seconds()\n",
    "\n",
    "final_score = final_results['score']\n",
    "improvement = final_score - initial_score\n",
    "\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Baseline Score: {initial_score:.1%} ({initial_results['passed']}/{initial_results['total']} passed)\")\n",
    "print(f\"Final Score:    {final_score:.1%} ({final_results['passed']}/{final_results['total']} passed)\")\n",
    "print(f\"Improvement:    {improvement:+.1%}\")\n",
    "print(f\"Target:         {TARGET_SCORE:.1%}\")\n",
    "print()\n",
    "print(f\"Duration: {val_duration:.1f}s\")\n",
    "print()\n",
    "\n",
    "if final_score >= TARGET_SCORE:\n",
    "    print(\"üéâ TARGET REACHED!\")\n",
    "elif improvement > 0:\n",
    "    print(f\"üìà IMPROVED by {improvement:+.1%} (gap to target: {TARGET_SCORE - final_score:.1%})\")\n",
    "elif improvement == 0:\n",
    "    print(\"‚ûñ NO CHANGE - fixes may not have addressed the failures\")\n",
    "else:\n",
    "    print(f\"üìâ REGRESSED by {improvement:.1%} - consider discarding changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what changed\n",
    "print(\"\\nDetailed comparison:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, (before, after) in enumerate(zip(initial_results['results'], final_results['results']), 1):\n",
    "    before_pass = before['passed']\n",
    "    after_pass = after['passed']\n",
    "    \n",
    "    if before_pass != after_pass:\n",
    "        if after_pass:\n",
    "            status = \"‚ùå‚Üí‚úÖ FIXED\"\n",
    "        else:\n",
    "            status = \"‚úÖ‚Üí‚ùå BROKEN\"\n",
    "        print(f\"{i}. {status}: {before['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£1Ô∏è‚É£ Promotion Decision\n",
    "\n",
    "**Your options:**\n",
    "1. **Promote** - Copy dev-working config to production\n",
    "2. **Discard** - Delete dev spaces, keep production unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*60)\n",
    "print(\"ENHANCEMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Production Space: {PRODUCTION_ID} (unchanged)\")\n",
    "print(f\"Dev-Working:      {DEV_WORKING_ID} (has fixes)\")\n",
    "print(f\"Dev-Best:         {DEV_BEST_ID} (backup)\")\n",
    "print()\n",
    "print(f\"Baseline Score: {initial_score:.1%}\")\n",
    "print(f\"Final Score:    {final_score:.1%}\")\n",
    "print(f\"Improvement:    {improvement:+.1%}\")\n",
    "print(f\"Fixes Applied:  {len(apply_result['applied'])}\")\n",
    "print()\n",
    "print(\"Your decision:\")\n",
    "print(\"  ‚Ä¢ Set CONFIRM_PROMOTE = True to apply changes to production\")\n",
    "print(\"  ‚Ä¢ Set CONFIRM_CLEANUP = True to discard and keep production unchanged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Promote to Production\n",
    "# This copies dev-working config to production\n",
    "\n",
    "CONFIRM_PROMOTE = False  # Set to True to enable\n",
    "\n",
    "if CONFIRM_PROMOTE:\n",
    "    print(\"Promoting dev-working to production...\")\n",
    "    \n",
    "    # Copy dev-working config to production\n",
    "    promote_result = space_cloner.copy_config(\n",
    "        source_space_id=DEV_WORKING_ID,\n",
    "        target_space_id=PRODUCTION_ID\n",
    "    )\n",
    "    \n",
    "    if promote_result['success']:\n",
    "        print(\"‚úÖ Production updated with enhanced configuration!\")\n",
    "        print(f\"   New score: {final_score:.1%}\")\n",
    "        \n",
    "        # Cleanup dev spaces\n",
    "        print(\"\\nCleaning up dev spaces...\")\n",
    "        cleanup_result = space_cloner.cleanup_dev_spaces()\n",
    "        if cleanup_result['success']:\n",
    "            print(\"‚úÖ Dev spaces deleted\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Cleanup warning: {cleanup_result['error']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Promotion failed: {promote_result['error']}\")\n",
    "else:\n",
    "    print(\"Set CONFIRM_PROMOTE = True to apply dev-working changes to production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Cleanup without promoting\n",
    "# This discards all changes and keeps production unchanged\n",
    "\n",
    "CONFIRM_CLEANUP = False  # Set to True to enable\n",
    "\n",
    "if CONFIRM_CLEANUP:\n",
    "    print(\"Cleaning up dev spaces (no changes to production)...\")\n",
    "    cleanup_result = space_cloner.cleanup_dev_spaces()\n",
    "    \n",
    "    if cleanup_result['success']:\n",
    "        print(\"‚úÖ Dev spaces deleted\")\n",
    "        print(\"Production space unchanged.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Cleanup warning: {cleanup_result['error']}\")\n",
    "else:\n",
    "    print(\"Set CONFIRM_CLEANUP = True to delete dev spaces and discard changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Debug Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON\n",
    "output = {\n",
    "    \"production_id\": PRODUCTION_ID,\n",
    "    \"dev_working_id\": DEV_WORKING_ID,\n",
    "    \"dev_best_id\": DEV_BEST_ID,\n",
    "    \"baseline_score\": initial_score,\n",
    "    \"final_score\": final_score,\n",
    "    \"improvement\": improvement,\n",
    "    \"fixes_applied\": len(apply_result['applied']),\n",
    "    \"fixes_failed\": len(apply_result['failed']),\n",
    "}\n",
    "\n",
    "with open('enhancement_result.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "print(\"‚úÖ Results saved to enhancement_result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export fixes to JSON for review\n",
    "with open('debug_fixes.json', 'w') as f:\n",
    "    json.dump(grouped_fixes, f, indent=2, default=str)\n",
    "print(\"‚úÖ Fixes saved to debug_fixes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Genie on dev-working\n",
    "test_question = \"What tables are available?\"\n",
    "print(f\"Testing Genie (dev-working): {test_question}\")\n",
    "\n",
    "response = genie_client.ask(test_question, timeout=60)\n",
    "print(f\"Status: {response['status']}\")\n",
    "if response.get('sql'):\n",
    "    print(f\"SQL: {response['sql'][:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}